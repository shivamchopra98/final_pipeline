import csv
import io
import json
import os
import re
from datetime import datetime
from typing import Any, Dict, List

OUT_FILENAME = "exploit_extract.json"

NULL_TOKENS = {"", "null", "none", "n/a", "na", "-"}

CVE_RE = re.compile(r"(CVE-\d{4}-\d{4,7})", flags=re.IGNORECASE)

def _norm_str(v: Any) -> str:
    if v is None:
        return None
    s = str(v).strip()
    if s.lower() in NULL_TOKENS:
        return None
    return s

def _maybe_int(v: Any):
    s = _norm_str(v)
    if s is None:
        return None
    if re.fullmatch(r"-?\d+", s):
        try:
            return int(s)
        except Exception:
            return s
    return s

def _split_list_field(v: Any, sep: str = ";") -> List[str]:
    s = _norm_str(v)
    if s is None:
        return []
    parts = [p.strip() for p in s.split(sep) if p.strip() and p.strip().lower() not in NULL_TOKENS]
    return parts

def _extract_cves_from_codes_field(codes_value: Any) -> List[str]:
    """
    Extract CVE identifiers (CVE-YYYY-NNNN...) from the 'codes' or similar column.
    Returns a list of uppercase CVE strings (may be empty).
    """
    s = _norm_str(codes_value)
    if not s:
        return []
    # common separators ; , | space
    tokens = re.split(r"[;,|\s]+", s)
    found = []
    for t in tokens:
        if not t:
            continue
        # find all CVE patterns inside token
        for m in CVE_RE.findall(t):
            if m:
                found.append(m.upper())
    # remove duplicates while preserving order
    seen = set()
    out = []
    for c in found:
        if c not in seen:
            seen.add(c)
            out.append(c)
    return out

def _parse_row_to_record(row: Dict[str, Any]) -> Dict[str, Any]:
    # normalize keys to strip
    r = {k.strip(): v for k, v in row.items()}

    def get(*candidates):
        for c in candidates:
            # direct
            if c in r and r[c] is not None:
                return r[c]
            # case-insensitive search
            for k in r.keys():
                if k.lower() == c.lower() and r[k] is not None:
                    return r[k]
        return None

    rec: Dict[str, Any] = {}
    # id as string (DynamoDB partition key)
    rec["id"] = _norm_str(get("id", "eid"))

    rec["codes"] = _norm_str(get("codes"))
    rec["date_updated"] = _norm_str(get("date_updated"))
    rec["aliases"] = _split_list_field(get("aliases"))
    rec["author"] = _norm_str(get("author"))
    rec["verified"] = _maybe_int(get("verified"))
    rec["description"] = _norm_str(get("description"))
    rec["type"] = _norm_str(get("type"))
    rec["application_url"] = _norm_str(get("application_url"))
    rec["platform"] = _norm_str(get("platform"))
    rec["source_url"] = _norm_str(get("source_url"))
    rec["tags"] = _split_list_field(get("tags"))
    rec["date_added"] = _norm_str(get("date_added"))
    rec["file"] = _norm_str(get("file"))
    rec["date_published"] = _norm_str(get("date_published"))
    rec["port"] = _maybe_int(get("port"))
    ud = _norm_str(get("uploaded_date"))
    rec["uploaded_date"] = ud if ud else datetime.utcnow().strftime("%Y-%m-%d")
    rec["screenshot_url"] = _norm_str(get("screenshot_url"))

    # PRIMARY NEW LOGIC: extract CVEs
    # 1) Try explicit cve-like columns first (if present)
    explicit_cve_field = get("CVE_id", "cve_id", "cves", "CVE", "cve")
    cves_from_explicit = []
    if explicit_cve_field:
        # allow semicolon/comma separated lists
        tokens = re.split(r"[;,|\s]+", str(explicit_cve_field))
        for tok in tokens:
            if not tok:
                continue
            m = CVE_RE.search(tok)
            if m:
                cves_from_explicit.append(m.group(1).upper())

    # 2) Also extract from 'codes' column
    cves_from_codes = _extract_cves_from_codes_field(rec.get("codes"))

    # Merge unique CVEs preserving order: explicit first then codes
    seen = set()
    merged = []
    for c in (cves_from_explicit + cves_from_codes):
        if c not in seen:
            seen.add(c)
            merged.append(c)

    rec["CVE_id"] = ";".join(merged) if merged else None

    return rec

def transform_csv(local_path: str) -> str:
    """
    Read a CSV (or CSV-like) file at local_path, normalize it and write JSON
    to ./daily_extract/exploit_extract.json. Returns that JSON path.
    """
    print(f"ðŸ”„ Transforming local ExploitDB file: {local_path}")

    # read raw bytes and decode robustly
    with open(local_path, "rb") as fh:
        raw_bytes = fh.read()
    text = None
    for enc in ("utf-8", "utf-8-sig", "latin1"):
        try:
            text = raw_bytes.decode(enc)
            break
        except Exception:
            text = None
    if text is None:
        raise ValueError("Unable to decode input file using utf-8 or latin1")

    # detect dialect (comma, tab, etc.)
    sample = text[:8192]
    try:
        dialect = csv.Sniffer().sniff(sample, delimiters=",\t;|")
    except Exception:
        dialect = csv.excel

    f = io.StringIO(text)
    reader = csv.DictReader(f, dialect=dialect)
    normalized = []
    row_count = 0
    for row in reader:
        row_count += 1
        rec = _parse_row_to_record(row)
        if not rec.get("id"):
            continue
        normalized.append(rec)

    # ensure output dir exists and write JSON
    out_dir = os.path.dirname(local_path) or "."
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, OUT_FILENAME)

    with open(out_path, "w", encoding="utf-8") as fh:
        json.dump(normalized, fh, indent=2, ensure_ascii=False)

    print(f"âœ… Transformed JSON written: {out_path} (records={len(normalized)}, raw_rows={row_count})")
    return out_path

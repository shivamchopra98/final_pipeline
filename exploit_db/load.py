# load_exploitdb.py
import json
import math
from decimal import Decimal
from typing import List, Dict, Any, Optional
from datetime import datetime
import boto3
from botocore.exceptions import ClientError

DEFAULT_CONFIG = {
    "TABLE_NAME": "infoservices-cybersecurity-vuln-exploit-data",
    "AWS_REGION": "us-east-1",
    "BATCH_PROGRESS_INTERVAL": 500,
    "BATCH_WRITE_CHUNK_SIZE": 500,
    "AWS_ACCESS_KEY_ID": None,
    "AWS_SECRET_ACCESS_KEY": None,
    "DDB_ENDPOINT": "",
}

VOLATILE_FIELDS = {"uploaded_date"}  # fields to ignore when checking for changes


def _resolve_cfg(user_cfg: Optional[Dict[str, Any]]):
    cfg = DEFAULT_CONFIG.copy()
    if user_cfg:
        cfg.update(user_cfg)
    return cfg


def _to_ddb_safe(v):
    if v is None:
        return None
    if isinstance(v, float):
        if math.isnan(v) or math.isinf(v):
            return None
        return Decimal(str(v))
    if isinstance(v, (int, Decimal)):
        return v
    if isinstance(v, (list, dict)):
        try:
            return json.dumps(v, sort_keys=True, ensure_ascii=False)
        except Exception:
            return str(v)
    s = str(v).strip()
    if s == "" or s.lower() in {"nan", "none"}:
        return None
    return s


def _date_only_from_any(s: Any) -> Optional[str]:
    if s is None:
        return None
    s = str(s).strip()
    if not s:
        return None
    if "T" in s:
        s = s.split("T", 1)[0]
    if " " in s:
        s = s.split(" ", 1)[0]
    if len(s) >= 10 and s[4] == "-" and s[7] == "-":
        return s[:10]
    # try parse common formats
    for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%d-%m-%Y", "%d/%m/%Y", "%m/%d/%Y"):
        try:
            dt = datetime.strptime(s[:10], fmt)
            return dt.strftime("%Y-%m-%d")
        except Exception:
            continue
    try:
        dt = datetime.fromisoformat(s)
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return None


def _parse_date_obj(s: Optional[str]):
    if not s:
        return None
    try:
        return datetime.strptime(s, "%Y-%m-%d").date()
    except Exception:
        try:
            d = _date_only_from_any(s)
            if d:
                return datetime.strptime(d, "%Y-%m-%d").date()
        except Exception:
            return None
    return None


def _max_date_from_ddb_table(table, date_field="date_updated") -> Optional[datetime.date]:
    """
    Scan the DynamoDB table (ProjectionExpression = date_field) and return max date object.
    """
    paginator = table.meta.client.get_paginator("scan")
    max_dt = None
    try:
        for page in paginator.paginate(TableName=table.name, ProjectionExpression=date_field):
            for itm in page.get("Items", []):
                val = itm.get(date_field)
                if not val:
                    continue
                dt = _parse_date_obj(val)
                if dt and (max_dt is None or dt > max_dt):
                    max_dt = dt
    except ClientError as e:
        print(f"‚ö†Ô∏è DynamoDB scan error when computing max date: {e}")
        raise
    return max_dt


def sync_exploit_records_to_dynamodb_and_s3(records: List[Dict[str, Any]], json_bytes: bytes, user_cfg: Dict[str, Any]) -> Dict[str, Any]:
    """
    DynamoDB-first incremental loader. DOES NOT fetch or upload S3 baseline.
    - Finds max(date_updated) in DynamoDB.
    - Writes only feed records with date_updated > max_date (or all if none).
    - Returns summary.
    """
    cfg = _resolve_cfg(user_cfg)

    ddb_kwargs = {"region_name": cfg.get("AWS_REGION")}
    if cfg.get("DDB_ENDPOINT"):
        ddb_kwargs["endpoint_url"] = cfg.get("DDB_ENDPOINT")
    # If you supplied AWS creds via env, boto3 will pick them up automatically.
    ddb = boto3.resource("dynamodb", **ddb_kwargs)

    # ensure table exists
    table_name = cfg.get("TABLE_NAME") or DEFAULT_CONFIG["TABLE_NAME"]
    existing_tables = ddb.meta.client.list_tables().get("TableNames", [])
    if table_name not in existing_tables:
        print(f"‚ö° Creating DynamoDB table '{table_name}'...")
        t = ddb.create_table(
            TableName=table_name,
            KeySchema=[{"AttributeName": "id", "KeyType": "HASH"}],
            AttributeDefinitions=[{"AttributeName": "id", "AttributeType": "S"}],
            ProvisionedThroughput={"ReadCapacityUnits": 5, "WriteCapacityUnits": 5}
        )
        t.meta.client.get_waiter("table_exists").wait(TableName=table_name)
        print("‚úÖ Table created.")
    table = ddb.Table(table_name)

    # compute max date_updated in DDB
    print(f"üîÅ Scanning existing records from DynamoDB table '{table_name}' for max 'date_updated' ...")
    try:
        max_date = _max_date_from_ddb_table(table, date_field="date_updated")
    except Exception:
        max_date = None

    if max_date:
        print(f"‚ÑπÔ∏è Current max 'date_updated' in DynamoDB: {max_date.isoformat()}")
    else:
        print("‚ÑπÔ∏è No existing 'date_updated' found in DynamoDB; treating as first-run (all feed rows considered new)")

    # build current map and decide which to write
    current_map = {str(r.get("id")): r for r in records if r.get("id")}
    total_current = len(current_map)
    print(f"‚ÑπÔ∏è Total records from feed: {total_current}")

    to_write = []
    skipped = 0
    for iid, rec in current_map.items():
        rec_date_str = rec.get("date_updated") or rec.get("uploaded_date")
        rec_date = _parse_date_obj(rec_date_str)
        if rec_date is None:
            # conservative: write records we cannot parse
            to_write.append(rec)
        else:
            if not max_date or rec_date > max_date:
                to_write.append(rec)
            else:
                skipped += 1

    print(f"üü° New/updated records to write: {len(to_write)}")
    print(f"‚ÑπÔ∏è Unchanged/skipped: {skipped}")

    # write to DynamoDB
    written = 0
    if to_write:
        print(f"‚¨ÜÔ∏è Writing {len(to_write)} items to DynamoDB...")
        with table.batch_writer() as batch:
            for i, rec in enumerate(to_write, start=1):
                item = {}
                for k, v in rec.items():
                    val = _to_ddb_safe(v)
                    if val is not None:
                        item[k] = val
                # ensure id exists and is string
                item["id"] = str(item.get("id") or rec.get("id"))
                # normalize date_updated
                dnorm = _date_only_from_any(rec.get("date_updated") or rec.get("uploaded_date"))
                if dnorm:
                    item["date_updated"] = dnorm
                try:
                    batch.put_item(Item=item)
                    written += 1
                except ClientError as e:
                    print(f"‚ùå Failed to write id={item.get('id')}: {e}")
                if i % cfg.get("BATCH_PROGRESS_INTERVAL", 500) == 0 or i == len(to_write):
                    print(f"‚¨ÜÔ∏è Batch wrote {i}/{len(to_write)} items")
        print(f"‚úÖ DynamoDB writes complete: uploaded={written}")
    else:
        print("‚ÑπÔ∏è Nothing to write to DynamoDB.")

    summary = {
        "total_records_feed": total_current,
        "new_or_updated": len(to_write),
        "written": written,
        "s3_baseline_uploaded": False,
    }
    print("‚ÑπÔ∏è Sync summary:", summary)
    return summary

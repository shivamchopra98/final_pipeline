import os
import json
import boto3
from botocore.exceptions import ClientError

DEFAULT_CONFIG = {
    "TABLE_NAME": "exploit_data",
    "DDB_ENDPOINT": "http://localhost:8000",
    "AWS_REGION": "us-east-1",
    "PROJECT_ROOT": os.getcwd(),
    "DAILY_DIR": None,
    "BASELINE_FILENAME": "exploitdb_extract.json",
    "BATCH_PROGRESS_SIZE": 100
}

def _resolve_config(user_config):
    cfg = DEFAULT_CONFIG.copy()
    if user_config:
        cfg.update(user_config)
    if not cfg["DAILY_DIR"]:
        cfg["DAILY_DIR"] = os.path.join(cfg["PROJECT_ROOT"], "daily_extract")
    cfg["BASELINE_FILE"] = os.path.join(cfg["DAILY_DIR"], cfg["BASELINE_FILENAME"])
    os.makedirs(cfg["DAILY_DIR"], exist_ok=True)
    return cfg

def get_dynamodb_table(cfg):
    # Use endpoint_url to point to local DynamoDB if provided (e.g. http://localhost:8000)
    ddb = boto3.resource(
        "dynamodb",
        region_name=cfg["AWS_REGION"],
        endpoint_url=cfg.get("DDB_ENDPOINT"),
    )
    existing = ddb.meta.client.list_tables().get("TableNames", [])
    if cfg["TABLE_NAME"] not in existing:
        print(f"⚡ Creating DynamoDB table '{cfg['TABLE_NAME']}' locally...")
        table = ddb.create_table(
            TableName=cfg["TABLE_NAME"],
            KeySchema=[{"AttributeName": "id", "KeyType": "HASH"}],
            AttributeDefinitions=[{"AttributeName": "id", "AttributeType": "S"}],
            ProvisionedThroughput={"ReadCapacityUnits": 5, "WriteCapacityUnits": 5}
        )
        table.meta.client.get_waiter("table_exists").wait(TableName=cfg["TABLE_NAME"])
        print("✅ Table created.")
    return ddb.Table(cfg["TABLE_NAME"])

def load_json_to_map(path):
    """Load list-of-dicts JSON into {id: record} map."""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    m = {}
    for rec in data:
        rid = rec.get("id")
        if not rid:
            continue
        m[str(rid).strip()] = rec
    return m

def items_equal(rec_a, rec_b):
    """Shallow comparison ignoring 'uploaded_date'."""
    if rec_b is None:
        return False
    for k, v in rec_a.items():
        if k == "uploaded_date":
            continue
        if rec_a.get(k) != rec_b.get(k):
            return False
    return True

def sync_today_with_dynamodb(current_json_path: str, config: dict = None):
    """
    Sync the transformed ExploitDB JSON (local path) with DynamoDB.
    - current_json_path: local path to the transformed JSON list
    - config: optional dict overriding DEFAULT_CONFIG
    Returns a summary dict.
    """
    cfg = _resolve_config(config)
    table = get_dynamodb_table(cfg)

    # load current transformed JSON
    current_map = load_json_to_map(current_json_path)
    total_current = len(current_map)
    print(f"ℹ️ Loaded current transformed records: {total_current}")

    # load baseline if present
    baseline_map = {}
    baseline_exists = os.path.exists(cfg["BASELINE_FILE"])
    if baseline_exists:
        baseline_map = load_json_to_map(cfg["BASELINE_FILE"])
        print(f"ℹ️ Baseline exists with {len(baseline_map)} records")
    else:
        print("ℹ️ No baseline found (first run)")

    # compute changed_ids (new or differing vs baseline)
    changed_ids = []
    for rid, rec in current_map.items():
        base_rec = baseline_map.get(rid)
        if base_rec is None or not items_equal(rec, base_rec):
            changed_ids.append(rid)

    if not changed_ids:
        print("✅ No new/updated records.")
        uploaded = 0
    else:
        print(f"⬆️ Writing {len(changed_ids)} items to DynamoDB...")
        uploaded = 0
        batch_size = cfg["BATCH_PROGRESS_SIZE"]
        with table.batch_writer(overwrite_by_pkeys=["id"]) as batch:
            for rid in changed_ids:
                rec = current_map[rid]
                # clean item: replace empty strings with None (Dynamo-friendly)
                safe_item = {}
                for k, v in rec.items():
                    if isinstance(v, str) and v.strip() == "":
                        safe_item[k] = None
                    else:
                        safe_item[k] = v
                safe_item["id"] = str(safe_item["id"])
                try:
                    batch.put_item(Item=safe_item)
                    uploaded += 1
                    if uploaded % batch_size == 0 or uploaded == len(changed_ids):
                        print(f"⬆️ Uploaded {uploaded}/{len(changed_ids)}")
                except ClientError as e:
                    print(f"❌ Failed to write id={safe_item.get('id')}: {e}")

    # Overwrite baseline with current authoritative data (atomic replace)
    try:
        abs_in = os.path.abspath(current_json_path)
        abs_base = os.path.abspath(cfg["BASELINE_FILE"])
        if abs_in != abs_base:
            os.replace(current_json_path, cfg["BASELINE_FILE"])
        print(f"✅ Baseline updated: {cfg['BASELINE_FILE']}")
    except Exception as e:
        print(f"❌ Failed to update baseline: {e}")
        raise

    summary = {
        "total_current": total_current,
        "changed_ids_considered": len(changed_ids),
        "to_write": len(changed_ids),
        "uploaded": uploaded,
        "baseline_file": cfg["BASELINE_FILE"],
        "table": cfg["TABLE_NAME"]
    }
    print("✅ Sync summary:", summary)
    return summary
